{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "w6K7xa23Elo4",
        "mDgbUHAGgjLW",
        "QHF8YVU7Yuh3",
        "bbFf2-_FphqN",
        "1M8mcRywphqQ",
        "X_VqEhTip1ck",
        "7wuGOrhz0itI",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "qjKvONjwE8ra",
        "VFOzZv6IFROw",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shasank55/online_retail_customer_segements/blob/main/ONLINE_RETAIL_customer_segements.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - \n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -** -shasank chawla"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the project where , our task is to identify major customer segments on a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.\n",
        "\n",
        "This Data Set contains 541909 number of rows and 8 number of columns.\n",
        "These are the coloumn:\n",
        "\n",
        "InvoiceNo\n",
        "\n",
        "StockCode\n",
        "\n",
        "Quantity\n",
        "\n",
        "InvoiceDate\n",
        "\n",
        "UnitPrice\n",
        "\n",
        "Customerid\n",
        "\n",
        "Country"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, your task is to identify major customer segments on a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.\n",
        "\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# Import all the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "from datetime import datetime\n",
        "import datetime as dt\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.metrics import silhouette_samples\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn import preprocessing\n",
        "from sklearn import metrics\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import scipy.cluster.hierarchy as sch\n",
        "from scipy.stats import shapiro, ttest_ind, f_oneway\n",
        "\n",
        "import seaborn as sns; sns.set()\n",
        "from prettytable import PrettyTable\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4U_8QGGb6icV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "online_retail=pd.read_csv(\"/content/Online Retail.xlsx - Online Retail (1).csv\")"
      ],
      "metadata": {
        "id": "0GwcsQSD6_e0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "online_retail.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "online_retail.tail()"
      ],
      "metadata": {
        "id": "Qpp5uxhQWLDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "online_retail.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "online_retail.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "len(online_retail[online_retail.duplicated()])"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "online_retail.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(14,7))\n",
        "sns.heatmap(online_retail.isnull(), cmap='bwr', yticklabels=False)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our dataset contains 102673 rows and 8 columns. Some values are missing in the Description column, and there are a large number of missing values in the CustomerID column. Additionally, we have identified 929 duplicate values within the dataset."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "online_retail.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "online_retail.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "our dataset contains 8 columns\n",
        "\n",
        "1.Invoice No\n",
        "\n",
        "2.StockCode\n",
        "\n",
        "3.Description\n",
        "\n",
        "4.Quantity\n",
        "\n",
        "5.InvoiceDate\n",
        "\n",
        "6.UnitPrice\n",
        "\n",
        "7.Customer\n",
        "\n",
        "8.IDCountry"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "online_retail.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis \n",
        "online_retail.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the sum of missing values to check whether any missing value is left or not\n",
        "online_retail.isnull().sum().sort_values()"
      ],
      "metadata": {
        "id": "JhDseT8PKiGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Droping the duplicate values from the dataset\n",
        "online_retail.drop_duplicates(inplace = True)"
      ],
      "metadata": {
        "id": "sqGrhU-1LPan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rows and column left after removing duplicate and null values\n",
        "print('Rows {} , Columns {}'.format(online_retail.shape[0], online_retail.shape[1]))"
      ],
      "metadata": {
        "id": "J8JSDsX_Lv-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we find the unique value of the dataset then we remove all the null values and then we remove all the duplicates and then we find the rows and column left after removing duplicates and null values"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Here we creating a copy of dataframe because whatever we applying a conditions on new data so that it doesn't affect our original dataset.**"
      ],
      "metadata": {
        "id": "32NJx9aEXXHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a copy of dataframe \n",
        "customer_seg_df = online_retail.copy()"
      ],
      "metadata": {
        "id": "8Rclf7VtXFBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**checking the rows which were cancelled**"
      ],
      "metadata": {
        "id": "STCXMaqOWxq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets check the InvoiceNo that starts with 'C'.Here 'C' denotes cancellation \n",
        "customer_seg_df[customer_seg_df['InvoiceNo'].str.contains(\"C\") == True]"
      ],
      "metadata": {
        "id": "reOqkhZ6W8hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here, column 'InvoiceNo' is of Object type**"
      ],
      "metadata": {
        "id": "ymGecggZXki8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch 'InvoiceNo' d-type\n",
        "customer_seg_df.InvoiceNo.dtypes"
      ],
      "metadata": {
        "id": "UtFrRzjeXqzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So,we need to convert it into 'str' type**\n"
      ],
      "metadata": {
        "id": "OhX35f1lXwK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# First converting the column as string\n",
        "customer_seg_df['InvoiceNo'] = customer_seg_df['InvoiceNo'].astype('str')"
      ],
      "metadata": {
        "id": "YuyvzwhfXz5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We have to deal with those data of InvoiceNo which not containing 'C'\n",
        "customer_seg_df = customer_seg_df[~customer_seg_df['InvoiceNo'].str.contains('C')]\n",
        "customer_seg_df"
      ],
      "metadata": {
        "id": "FASgnRk5YBe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rows and column left after removing duplicate and null values\n",
        "print('Rows {} , Columns {}'.format(customer_seg_df.shape[0], customer_seg_df.shape[1]))"
      ],
      "metadata": {
        "id": "f_p2DGbHYJJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now,the shape of that are not the dataframe got changed which contain only those transaction that are not cancelled**"
      ],
      "metadata": {
        "id": "Ko1iZgpUYNuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the no. of rows in each column\n",
        "customer_seg_df.count()"
      ],
      "metadata": {
        "id": "qng6DLxeYQEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**NOW our dataset is ready for all the analysis**"
      ],
      "metadata": {
        "id": "ZpNnHVD3YXj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Top 5 countries who's buying maximum products\n",
        "Top_5_country = customer_seg_df['Country'].value_counts().reset_index()\n",
        "Top_5_country['Country_per']= Top_5_country['Country']*100/customer_seg_df['Country'].count()  \n",
        "# Add country percentage column to top 10 countries data frame\n",
        "Top_5_country.rename(columns={'index': 'Country_Name','Country': 'Count'}, inplace=True)\n",
        "Top_5_country.head()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Barplot of Top 5 countries who's buying maximum products\n",
        "fig, ax = plt.subplots(figsize = (10,10))\n",
        "splot = sns.barplot(x='Country_Name',y='Count',data=Top_5_country.head())\n",
        "plt.title('% of Order from each Country', SIZE = 18)\n",
        "plt.xlabel('Countries', SIZE = 18 , fontweight = 'bold')\n",
        "plt.ylabel('Count', SIZE = 18, fontweight = 'bold')\n",
        "plt.xticks(fontweight = 'bold' , fontsize = 12)\n",
        "plt.yticks(fontweight = 'bold' , fontsize = 12)\n",
        "for p in splot.patches:                               # This step is used for showing the percentage on the graph\n",
        "    height = p.get_height()\n",
        "    splot.text(p.get_x()+p.get_width()/2, height+500, '{:1.2f}''%'.format(height/customer_seg_df.shape[0]*100),ha = \"center\", SIZE = 15)"
      ],
      "metadata": {
        "id": "AR2Ujx-mgKFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar plots allow easy comparison of values for different categories or groups."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maximum orders are recieved from United Kingdom (Around 90% market covering).Since,company is a UK based online retail. After UK the top-most countries from where orders received are Germany, France, EIRE, etc"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maximum orders are recieved from United Kingdom (Around 90% market covering).Since,company is a UK based online retail.\n",
        "After UK the top-most countries from where orders received are Germany, France, EIRE, etc.The gained insights can potentially help in creating a positive business impact by identifying the top performing countries and focusing on strategies to increase sales in those markets. "
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Find the top 5 products which have maximum sales.**"
      ],
      "metadata": {
        "id": "di3_G4xQhgi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Create a variable and check the description variable\n",
        "Description_data = online_retail['Description'].value_counts().reset_index()\n",
        "Description_data.rename(columns = {'index' : 'Description_Name'}, inplace = True)\n",
        "Description_data.rename(columns = {'Description' : 'Count'}, inplace = True)\n",
        "Description_data.head(5)"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting top 5 product:\n",
        "plt.figure(figsize=(18,6))\n",
        "plt.title('Top Five Product', fontsize=15)\n",
        "sns.barplot(x='Description_Name', y='Count', data = Description_data.head(5))"
      ],
      "metadata": {
        "id": "JI898Od_h4vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar plots allow easy comparison of values for different categories or groups."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows the count of different product names, and the most popular product is the \"WHITE HANGING HEART T-LIGHT HOLDER\" with 494 counts, followed by \"REGENCY CAKESTAND 3 TIER\" with 403 counts."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " there are many insights leads to posyive growth but there is one insight that could lead to negative growth is if the most popular products are not profitable or have a low profit margin. For example, if the top-selling product is the \"WHITE HANGING HEART T-LIGHT HOLDER\" but the profit margin on this item is low, it could negatively impact the business's profitability. It would be important to consider additional information such as the cost of goods sold and overall sales revenue to make a more informed decision."
      ],
      "metadata": {
        "id": "IDjTn7QciLOB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# Plotting bottom 5 product:\n",
        "plt.figure(figsize=(18,6))\n",
        "plt.title('Bottom Five Product', fontsize=15)\n",
        "sns.barplot(data = Description_data[-5:], x='Description_Name', y='Count',palette ='magma')"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar plots can be customized to show different information, such as percentages or raw counts."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the insights show that these are the bottom 5 product and less sale product"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "there is neagtive impact is that we should focus on the product and the marketing of the product and make it avalilable eaisly."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Checking the number of cancelled orders country-wise**"
      ],
      "metadata": {
        "id": "nvfZn5-QkfS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Let us now look at the number of cancelled orders in the data.\n",
        "# Boolean series returned with False at place of NaN\n",
        "bool_series = online_retail[\"InvoiceNo\"].str.startswith('C', na=False)\n",
        "# Displaying filtered dataframe\n",
        "cancel_order = online_retail[bool_series]\n",
        "cancel_order[:10]"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch cancelled orders records and features shape\n",
        "cancel_order.shape"
      ],
      "metadata": {
        "id": "Od4lr6_sk3c1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the top 10 countries in which no. of cancelled orders are maximum\n",
        "\n",
        "cancel_order_country_top10 = cancel_order.groupby('Country')['InvoiceNo'].count().reset_index()\n",
        "cancel_order_country_top10 = cancel_order_country_top10.sort_values('InvoiceNo',ascending=False, ignore_index=True)\n",
        "cancel_order_country_top10.rename(columns={'InvoiceNo': 'Cancel_Order'}, inplace=True)\n",
        "cancel_order_country_top10.head(10)"
      ],
      "metadata": {
        "id": "HJmuZ3M9lKu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Plot the line graph of seaborn library to represent adr monthly for each hotel type\n",
        "#  x and y are columns to be plot on graph \n",
        "#  Data is name of dataframe \n",
        "#  Hue parameter represents which column in the data frame, you want to use for color encoding.\n",
        "#  Palette parameter is used for different color patterns\n",
        "#  Hue_order represents the order of hotel shown by bars\n",
        "plt.figure(figsize = (12,8))\n",
        "sns.pointplot(x = \"Country\", y=\"Cancel_Order\", data=cancel_order_country_top10[:10],palette= 'Set1')\n",
        "plt.title(\"Maximum orders cancelled from each country\", weight = 'bold') #  set the title of graph\n",
        "plt.xlabel(\"Country\" , weight='bold')  #  label at x-axis with font style as bold\n",
        "plt.xticks(rotation = 45 , fontweight = 'bold' , fontsize = 12) # rotate the x-axis label by 45 degree angle\n",
        "plt.yticks(fontweight = 'bold' , fontsize = 12)\n",
        "plt.ylabel(\"Cancel Order\" , weight='bold')   #  label at y-axis with font style as bold\n",
        "\n",
        "#  to show a graph.It is necessary when you are using Matplotlib in terminal and optional when you are using notebook\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p84jdylvlNz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Point plots allow us to easily compare the means of a numerical variable across different levels of a categorical variable."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows the countries with the highest number of cancelled orders. The United Kingdom had the most cancellations, followed by Germany and EIRE. This information could help the business identify areas for improvement in their online retail process, such as clearer communication and product descriptions. Further investigation could also reveal patterns or reasons for the high number of cancellations in certain countries."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the insights gained from the given data will create a positive business impact. However, one insight that could lead to negative growth is if there is a high number of cancelled orders in a particular country, as this may indicate issues with the product or delivery processes in that country. It would be important for the business to investigate the reasons for the high number of cancelled orders and take appropriate actions to address the underlying issues."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Create a new column with the Total_Price paid by the customer:\n",
        "online_retail['Total_Price'] = online_retail['Quantity']*online_retail['UnitPrice']\n",
        "online_retail.head()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top most Quantity and total price paid by the customer:\n",
        "max_price=online_retail['Total_Price'].value_counts().reset_index().rename(columns = {'index':'Quantity'})\n",
        "max_price=max_price.sort_values(by='Total_Price',ascending=False).head(20)\n",
        "print(max_price)"
      ],
      "metadata": {
        "id": "7hlT77R8rbM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#seaborn Bar plot Top most Quantity and total price paid by the customer:\n",
        "plt.figure(figsize=(16,6))\n",
        "sns.barplot(data=max_price, x=\"Total_Price\",y=\"Quantity\",palette = ('hsv'))\n",
        "plt.xticks(rotation=90)\n",
        "plt.fontsize=12\n",
        "plt.title('Top most Quantity and total price paid by the customer')"
      ],
      "metadata": {
        "id": "smX8ETFYrqii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar plots allow us to easily compare the magnitude of different categories by visually comparing the height or length of the bars."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows the top 20 quantities sold along with their corresponding total prices. The insight from this chart is that there is no clear correlation between quantity and total price - some high-quantity items are relatively inexpensive, while some lower-quantity items are quite expensive."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The top 20 quantities sold do not have a clear correlation between quantity and total price, suggesting a diverse range of products with different price points selling well. This insight can help optimize pricing and inventory strategies and have a positive business impact. No insights lead to negative growth as the data does not indicate any specific issues or challenges."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "# Analysing Transactions each Country:\n",
        "Countries_data = online_retail['Country'].value_counts().reset_index()\n",
        "Countries_data.rename(columns = {'index' : 'Country_Name'}, inplace = True)\n",
        "Countries_data.rename(columns = {'Country' : 'Count'}, inplace = True)\n",
        "\n",
        "# Getting 5 Highest transactions Countries:\n",
        "Countries_data.head()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the top 5 countries:\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title('Top 5 Country & Count',fontsize=20)\n",
        "sns.barplot(x='Country_Name',y='Count',data=Countries_data[:5], palette='hot_r')"
      ],
      "metadata": {
        "id": "zVpDaxMx-6rT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar plots can be used to show how the magnitudes of different categories change over time by using multiple bars for each time period."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems that the vast majority of online retail customers are from the United Kingdom, with Germany, France, EIRE, and Spain rounding out the top five countries. "
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows the top five countries with the highest number of orders, with the United Kingdom having by far the highest count. The insight from this chart is that the majority of the business's orders are coming from the UK, suggesting that this is a key market for the business. The gained insights could potentially have a positive business impact by helping the business to focus its marketing and sales efforts on the UK market, and to tailor its products and services to meet the specific needs and preferences of UK customers. There are no insights that necessarily lead to negative growth, as the data does not indicate any specific issues or challenges that need to be addressed."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "#Getting 5 Lowest transactions Countries:\n",
        "Countries_data.tail()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the Lowest 5 countries\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title('Top 5 Country & Count',fontsize=20)\n",
        "sns.barplot(x='Country_Name',y='Count',data=Countries_data[-5:], palette='PuBuGn')"
      ],
      "metadata": {
        "id": "byQ1UTKz__I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Saudi Arabia has the lowest purchasing history, so we won't concentrate more on these five nations while analyzing the consumer."
      ],
      "metadata": {
        "id": "NkzKaJv_A2I4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar plots can be used to highlight differences or similarities between categories, which can be useful for identifying patterns or trends in the data."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Saudi Arabia has the lowest purchasing history, so we won't concentrate more on these five nations while analyzing the consumer."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows the counts of orders from various countries, with Greece, Austria, United Arab Emirates, Israel, and Saudi Arabia having relatively low order counts. The insights gained from this chart may help the business to focus on the countries with the highest order counts and potentially develop strategies to increase orders from countries with lower counts. This could have a positive impact on the business by allowing it to allocate its resources more efficiently and potentially increase revenue. However, there are no insights that necessarily lead to negative growth, as the data does not indicate any specific issues or challenges that need to be addressed."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "# Getting StockCodes_Name with highest selling:\n",
        "StockCode_Df=online_retail['StockCode'].value_counts().reset_index()\n",
        "StockCode_Df.rename(columns={'index': 'StockCode_Names'}, inplace=True)\n",
        "StockCode_Df.rename(columns={'StockCode': 'Count'}, inplace=True)\n",
        "StockCode_Df.head(10)\n",
        "     "
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting StockCodes_Name with highest selling:\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.title('Top 5 Stock Names')\n",
        "sns.barplot(x='StockCode_Names',y='Count',data=StockCode_Df[:10], palette='PuOr')"
      ],
      "metadata": {
        "id": "mGUgMUx9BPgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar plots can be used to highlight differences or similarities between categories, which can be useful for identifying patterns or trends in the data."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows the top 10 stock codes by count, indicating the most frequently purchased items. The insight from this chart is that there are several products that are significantly more popular than others."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows the top 10 StockCode names along with their corresponding count of purchases. The insight from this chart is that a relatively small number of items make up a significant portion of the purchases, which could suggest that these items are popular and in high demand. The gained insights could potentially have a positive business impact by helping the business to identify popular products and optimize pricing and inventory strategies for these items. There are no insights that necessarily lead to negative growth, as the data does not indicate any specific issues or challenges that need to be addressed."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**What is the average number of orders per customer?**"
      ],
      "metadata": {
        "id": "vL1Mdf7gBgTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "# Customer distribution in 1 year time period\n",
        "customer_distribution = customer_seg_df.groupby('InvoiceDate')['CustomerID'].count().reset_index(name=\"Total_Customers\")\n",
        "customer_distribution"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot scatter graph to show no. of customers who are buying prodcut over a time-period of 1 year\n",
        "customer_distribution.plot.line(x='InvoiceDate', y='Total_Customers')\n",
        "plt.title('Distribution of customers Over period of 1 year', size=25)\n",
        "plt.xlabel('Invoice date', size=18)\n",
        "plt.ylabel('No. of Customers', size=18)\n",
        "plt.xticks(fontweight = 'bold' , fontsize = 12)\n",
        "plt.yticks(fontweight = 'bold' , fontsize = 12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z5HpMtmlNb4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Scatter plots can be useful for exploratory data analysis, helping us to understand the distribution of values and identify potential relationships between variables."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows the total number of customers who made purchases for each invoice date. The insights from this chart could include identifying trends and patterns in customer behavior over time."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "he chart shows the number of customers who made purchases for each invoice date. Insights could include identifying trends and patterns in customer behavior over time, which could help optimize operations and improve customer satisfaction.From the above line plot,we have seen that:\n",
        "There are more no. of customers between Oct,2011 to Nov,2011.\n",
        "Customers are fluctuating between Jan,2011 to Sep,2011."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Checking the number of cancelled orders country-wise**"
      ],
      "metadata": {
        "id": "00paOb0WNoS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "  # Extract the date from datetime\n",
        "def extract_date(datetime_obj):\n",
        "  return datetime_obj.date()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "def extract_date(datetime_obj):\n",
        "    if isinstance(datetime_obj, datetime.datetime):\n",
        "        return datetime_obj.date()\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "customer_seg_df['date'] = pd.to_datetime(customer_seg_df['InvoiceDate'], errors='coerce').apply(extract_date)"
      ],
      "metadata": {
        "id": "8HVEH-iPR9aP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# head gives the top 5 rows of the given dataset\n",
        "customer_seg_df.head()"
      ],
      "metadata": {
        "id": "r-zhiQhYR_bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grouping features 'CustomerID' and 'date' with 'Quantity' and sorting values by 'Quantity'\n",
        "qtywise_ctr= customer_seg_df.groupby(['CustomerID','date'])['Quantity'].sum().reset_index()\n",
        "qtywise_ctr.sort_values(by='Quantity', ascending=False).head()"
      ],
      "metadata": {
        "id": "91UT7kR5SFcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# describe() gives the overall descriptive summary of the dataframe\n",
        "qtywise_ctr['Quantity'].describe()"
      ],
      "metadata": {
        "id": "noZwS3NTSK7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descriptive statistics plot\n",
        "describe_qnty_df = qtywise_ctr.describe()\n",
        "describe_qnty_df.reset_index(inplace=True)\n",
        "describe_qnty_df = describe_qnty_df[describe_qnty_df['index'] != 'count']\n",
        "sns.factorplot(x='index', y='Quantity', data=describe_qnty_df,fontweight=\"bold\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cHe6kWhvSOGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Average quantity customers purchase per order\n",
        "data3= qtywise_ctr[qtywise_ctr['Quantity']<25930.0]\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.distplot(data3['Quantity'])\n",
        "plt.xlabel(\"Quantity\")\n",
        "xmean=qtywise_ctr['Quantity'].mean()\n",
        "plt.axvline(x=xmean, color='red', label= xmean)\n",
        "plt.legend()\n",
        "plt.title(\"Average quantity customers purchase per order\");"
      ],
      "metadata": {
        "id": "2L9wKtDMSVYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distribution plots provide a visual representation of the distribution of a numerical variable, allowing us to see how the values are spread out across the range of the variable."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the insights is The Average quantity customers purchase per order distribution of Monetary value is also skewed."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insights gained from the chart could help the business to identify the average quantity of products customers typically purchase per order. This information could be used to optimize inventory and pricing strategies to better meet customer demand and preferences, potentially leading to a positive impact on business growth. There are no insights that necessarily lead to negative growth, as the data does not indicate any specific issues or challenges that need to be addressed."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**What is the average number of orders per customer?**"
      ],
      "metadata": {
        "id": "hsJ7OXyySmP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "# Average quantity\n",
        "avg_qnty= qtywise_ctr.groupby(['CustomerID'],as_index=False).agg({'Quantity':'mean'}).rename(columns={'Quantity':'qnty/order'})\n",
        "orders_df= qtywise_ctr['CustomerID'].value_counts().reset_index().rename(columns={'index':'CustomerID', 'CustomerID':'orders'}).sort_values(by='CustomerID')\n",
        "avg_qnty= avg_qnty.merge(orders_df)\n",
        "avg_qnty.head()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# describe() gives the overall descriptive summary of the dataframe\n",
        "avg_qnty['orders'].describe()"
      ],
      "metadata": {
        "id": "IR-7HRUdSsCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Factorplot for 'index' and 'orders'\n",
        "describe_orders_df = avg_qnty.describe()\n",
        "describe_orders_df.reset_index(inplace=True)\n",
        "describe_orders_df = describe_orders_df[describe_orders_df['index'] != 'count']\n",
        "sns.factorplot(x='index', y='orders', data=describe_orders_df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KRhnN5KnSx91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Average number of orders per customer\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.distplot(avg_qnty['orders'])\n",
        "plt.xlabel(\"Number of Orders\")\n",
        "xmean=avg_qnty['orders'].mean()\n",
        "plt.axvline(x=xmean, color='red', label= xmean)\n",
        "plt.legend()\n",
        "plt.title(\"Average number of orders per customer\");"
      ],
      "metadata": {
        "id": "h6mSqtGASy1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distribution plots can also help us to identify the modes or peaks in the data, as well as any outliers or unusual values."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the insights from the chart is Customers have placed orders 4 or 5 times on average and maximum number of orders being 146 from a single customer."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight from the chart is that the average number of orders per customer is relatively low, with a long tail of high-frequency customers. This could indicate an opportunity to increase customer loyalty and encourage repeat purchases through targeted marketing and customer engagement efforts. By identifying and rewarding high-frequency customers, the business may be able to improve customer retention and drive revenue growth. There are no insights that necessarily lead to negative growth, as the data does not indicate any specific issues or challenges that need to be addressed."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Checking the distribution of numerical column to understand the data distribution is whether positively skewed, negative skewed or symetric.**"
      ],
      "metadata": {
        "id": "NWCvatVWTC1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From the above plot , We are not find anything from this type of graph so we use log transformation it basically shrink the data from large to small scale**"
      ],
      "metadata": {
        "id": "YrrGzCZeTIc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "# Log transformation on distribution of Quantity \n",
        "plt.figure(figsize=(12,7))\n",
        "plt.title('log distribution of Quantity', SIZE =18)\n",
        "sns.distplot(np.log(customer_seg_df['Quantity']),color=\"Orange\")\n",
        "plt.xticks(fontweight = 'bold' , fontsize = 12) # rotate the x-axis label by 45 degree angle\n",
        "plt.yticks(fontweight = 'bold' , fontsize = 12)"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Now it is much better to understand the distibution of quantity column"
      ],
      "metadata": {
        "id": "8us0vTOETSMz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph is a histogram with a kernel density estimate, showing the log-transformed distribution the chart was created to display the distribution of the logarithm of the quantity of items purchased by customers. It is useful because it can help to visualize the distribution of quantity "
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from this chart could help the business to identify customers who make large purchases, and target them with personalized marketing or promotional strategies."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The log transformation on the distribution of Quantity allows for a better understanding of the spread of data. The insights gained from this chart could help the business to identify customers who make large purchases, and target them with personalized marketing or promotional strategies. Additionally, the business could use this information to adjust inventory or pricing strategies to accommodate high-volume customers. Overall, the insights could help to improve customer satisfaction and drive positive business impact. There are no insights that necessarily lead to negative growth."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Find the sales within a week per day and find which day have maximum sales.**\n",
        "**We need to convert InvoiceDate column into proper date time format**\n",
        "\n",
        "**Create new column for day ,month ,year ,hour ,minute**"
      ],
      "metadata": {
        "id": "hn-PlFLATkHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Convert InvoiceDate columns into date time format**"
      ],
      "metadata": {
        "id": "5wVTWvzqT4tz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "# Extracting new feature from Invoice Date\n",
        "customer_seg_df[\"InvoiceDate\"] = pd.to_datetime(customer_seg_df[\"InvoiceDate\"], format=\"%m/%d/%y %H:%M\")\n",
        "customer_seg_df['Day'] = customer_seg_df['InvoiceDate'].dt.day_name()\n",
        "customer_seg_df[\"year\"] = customer_seg_df[\"InvoiceDate\"].apply(lambda x: x.year)\n",
        "customer_seg_df[\"month_num\"] = customer_seg_df[\"InvoiceDate\"].apply(lambda x: x.month)\n",
        "customer_seg_df[\"day_num\"] = customer_seg_df[\"InvoiceDate\"].apply(lambda x: x.day)\n",
        "customer_seg_df[\"hour\"] = customer_seg_df[\"InvoiceDate\"].apply(lambda x: x.hour)\n",
        "customer_seg_df[\"minute\"] = customer_seg_df[\"InvoiceDate\"].apply(lambda x: x.minute)\n",
        "customer_seg_df['Month']=customer_seg_df['InvoiceDate'].dt.month_name()\n",
        "customer_seg_df.head()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So,we have created mutiple columns and seperate day , month , year ,hour and minute from InvoiceNo column**\n",
        "\n",
        "**Now ,we can explore the daily and monthly sales.**"
      ],
      "metadata": {
        "id": "_K8mr-7gVrP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking sales day-wise**"
      ],
      "metadata": {
        "id": "TlBi2kEPV9du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# On which day most of the order is placed \n",
        "sort_order_week = [\"Sunday\", \"Monday\" , \"Tuesday\" , \"Wednesday\" , \"Thursday\" , \"Friday\" , \"Saturday\"]\n",
        "day_df = pd.DataFrame(customer_seg_df['Day'].value_counts().reset_index().values,columns=['Week-Day','Total-Sales'])\n",
        "day_df.index = pd.CategoricalIndex(day_df['Week-Day'],categories=sort_order_week,ordered=True)\n",
        "day_df = day_df.sort_index().reset_index(drop=True)\n",
        "day_df"
      ],
      "metadata": {
        "id": "9UgDUAtFV5mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.pylabtools import figsize\n",
        "# Draw pie chart to shoe the percentage of sales per day\n",
        "# A wedge of a pie chart can be made to explode from the rest of the wedges of the pie chart using the explode parameter of the pie function.\n",
        "\n",
        "explode = (0.03,0.05,0.05,0.05,0.05,0.02)\n",
        "labels = [\"Sunday\", \"Monday\" , \"Tuesday\" , \"Wednesday\" , \"Thursday\" , \"Friday\"]\n",
        "\n",
        "# Startangle parameter is used to rotatate a pie chart by various angles\n",
        "# Autopct parameter enables you to display the percent value using Python string formatting  \n",
        "# Textprops parameter is used to change color and fontsize of values inside pie chart\n",
        "\n",
        "day_df.plot.pie( y=\"Total-Sales\",figsize=(10,10),explode=explode, autopct='%1.1f%%',labels=labels, textprops={'color':\"w\",'fontsize':14})\n",
        "plt.title('Proportion of sales daily', weight='bold')  #  title to represent graph objective"
      ],
      "metadata": {
        "id": "iUuuO2pMWJ7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pie charts are useful for showing how the whole is divided into different parts or proportions. This can be helpful when we want to communicate the relative size of different categories or subgroups."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the insights from pie chart is There is no sale on Saturday.\n",
        "Thursday have highest sale followed by Wednesday and Tuesday."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from this chart could help create a positive business impact by identifying which days of the week have the highest and lowest total sales. This could enable the business to adjust its marketing or promotional strategies to increase sales on the lower performing days. For example, the business may offer promotions or discounts on those days to incentivize customers to make purchases. However, it is important to note that there may be other factors influencing sales, such as holidays or seasonal trends, and these should be taken into account when making strategic decisions. Overall, the insights from this chart can help optimize operations and increase revenue.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "# Plotting heatmap to show correlation of different variables\n",
        "plt.figure(figsize=(13,6.5))\n",
        "correlation = online_retail.corr()\n",
        "sns.heatmap(abs(correlation), annot = True, cmap = \"icefire\")"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heatmaps are often used to visualize complex data and relationships between variables. They are useful in identifying patterns, trends, and correlations between variables, especially when there are large amounts of data to be analyzed"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correlation matrix shows that there is a strong positive correlation between Quantity and Total_Price, and weak negative correlations between Quantity/CustomerID, UnitPrice/CustomerID, and UnitPrice/Total_Price. This suggests that changes in Quantity have a strong impact on Total_Price, while there is no clear relationship between the other variables."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot "
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(online_retail,kind=\"scatter\")"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pair plot is a type of visualization that is used to explore the relationships between pairs of variables in a dataset. It is a useful tool for understanding the correlation between different variables and identifying potential patterns and trends in the data."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "here we can see all the coreleted columns making the chart with each other 16 chart for four column with each other where strong positive correlation between Quantity and Total_Price, and weak negative correlations between Quantity/CustomerID, UnitPrice/CustomerID, and UnitPrice/Total_Price. This suggests that changes in Quantity have a strong impact on Total_Price."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothetical statements:\n",
        "\n",
        "1.The log-transformed distribution of Quantity follows a normal distribution.\n",
        "\n",
        "2.There is a significant difference in the mean log-transformed Quantity between the UK and Germany.\n",
        "\n",
        "3.There is no significant difference in the mean log-transformed Quantity between the different payment methods.\n"
      ],
      "metadata": {
        "id": "guvl--kkA-dG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test these hypotheses, we will perform the following statistical tests:\n",
        "\n",
        "1.Shapiro-Wilk test for normality\n",
        "\n",
        "2.Independent t-test\n",
        "\n",
        "3.One-way ANOVA\n",
        "\n",
        "(Note: we will assume a significance level of 0.05 for all tests.)"
      ],
      "metadata": {
        "id": "deemHDr_htH4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis 1:\n",
        "Research hypothesis: The log-transformed distribution of Quantity does not follow a normal distribution.\n",
        "\n",
        "Null hypothesis: The log-transformed distribution of Quantity follows a normal distribution.\n",
        "\n",
        "Alternative hypothesis: The log-transformed distribution of Quantity does not follow a normal distribution."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Shapiro-Wilk test for normality\n",
        "stat, p = shapiro(np.log(customer_seg_df['Quantity']))\n",
        "print(\"Shapiro-Wilk test for normality:\\nStatistic =\", stat, \"\\np-value =\", p)"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test whether the log-transformed distribution of Quantity follows a normal distribution, I used the Shapiro-Wilk test for normality."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test whether the log-transformed distribution of Quantity follows a normal distribution, I chose the Shapiro-Wilk test for normality because it is a widely used test for normality and is more powerful than other normality tests, especially for small sample sizes. This test is appropriate for our data because we are interested in testing whether the log-transformed distribution of Quantity follows a normal distribution or not, and the Shapiro-Wilk test specifically tests for normality."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis 2:\n",
        "Research hypothesis: There is a significant difference in the mean log-transformed Quantity between the UK and Germany.\n",
        "\n",
        "Null hypothesis: There is no significant difference in the mean log-transformed Quantity between the UK and Germany.\n",
        "\n",
        "Alternative hypothesis: There is a significant difference in the mean log-transformed Quantity between the UK and Germany."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Filter the data for UK and Germany\n",
        "uk_log_quantity = np.log(customer_seg_df.loc[customer_seg_df['Country'] == 'United Kingdom']['Quantity'])\n",
        "germany_log_quantity = np.log(customer_seg_df.loc[customer_seg_df['Country'] == 'Germany']['Quantity'])\n",
        "\n",
        "# Independent t-test\n",
        "stat, p = ttest_ind(uk_log_quantity, germany_log_quantity, equal_var=False)\n",
        "print(\"Independent t-test:\\nStatistic =\", stat, \"\\np-value =\", p)\n",
        "\n",
        "# Conclusion\n",
        "if p > 0.05:\n",
        "    print(\"There is no significant difference in the mean log-transformed Quantity between the UK and Germany.\")\n",
        "else:\n",
        "    print(\"There is a significant difference in the mean log-transformed Quantity between the UK and Germany.\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test whether there is a significant difference in the mean log-transformed Quantity between the UK and Germany, I used an independent t-test."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test whether there is a significant difference in the mean log-transformed Quantity between the UK and Germany, I chose an independent t-test because we are comparing the means of two independent samples (UK and Germany), and the log-transformed Quantity data appear to be normally distributed. The independent t-test is appropriate for this situation because it tests whether the means of two independent groups are statistically different or not."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis 3:\n",
        "Research hypothesis: There is a significant difference in the mean log-transformed Quantity between the different payment methods.\n",
        "Null hypothesis: There is no significant difference in the mean log-transformed Quantity between the different payment methods.\n",
        "Alternative hypothesis: There is a significant difference in the mean log-transformed Quantity between the different payment methods."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Null hypothesis: There is no significant difference in the mean log-transformed Quantity between the different countries.\n",
        "# Alternative hypothesis: There is a significant difference in the mean log-transformed Quantity between the different countries.\n",
        "# Set significance level alpha (e.g., 0.05)\n",
        "alpha = 0.05\n",
        "\n",
        "# Extract unique countries from the dataframe\n",
        "countries = customer_seg_df['Country'].unique()\n",
        "\n",
        "# Create a list of groups for each country, where each group contains the log-transformed quantity values\n",
        "groups = [np.log(customer_seg_df.loc[customer_seg_df['Country'] == country]['Quantity']) for country in countries]\n",
        "\n",
        "# Perform one-way ANOVA test to compare the means of the groups\n",
        "statistic, p_value = f_oneway(*groups)\n",
        "\n",
        "# Print the results\n",
        "print(f\"One-way ANOVA:\\nStatistic = {statistic}, p-value = {p_value}\")\n",
        "\n",
        "# Check if p-value is less than significance level alpha\n",
        "if p_value < alpha:\n",
        "    print(\"Reject null hypothesis. There is a significant difference in the mean log-transformed Quantity between the different countries.\")\n",
        "else:\n",
        "    print(\"Fail to reject null hypothesis. There is no significant difference in the mean log-transformed Quantity between the different countries.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test whether there is a significant difference in the mean log-transformed Quantity between the different payment methods, I used a one-way ANOVA."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANOVA tests whether there is a significant difference among the means of the groups, and it calculates the F-statistic and p-value to determine whether to reject or fail to reject the null hypothesis. If the p-value is less than the chosen significance level (e.g., 0.05), we reject the null hypothesis and conclude that there is a significant difference in the means of the groups. If the p-value is greater than the significance level, we fail to reject the null hypothesis and conclude that there is not enough evidence to suggest a significant difference in the means of the groups."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "online_retail.isnull().sum()\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "#Plotting graph Quantity & UnitPrice:\n",
        "plt.figure(figsize=(14,7))\n",
        "plt.subplot(2,1,1)\n",
        "sns.boxplot(online_retail['Quantity'])\n",
        "plt.subplot(2,1,2)\n",
        "sns.boxplot(online_retail['UnitPrice'])\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "# Define function to remove outliers using Tukey's fences\n",
        "def remove_outliers(data, column):\n",
        "    Q1 = np.percentile(data[column], 25)\n",
        "    Q3 = np.percentile(data[column], 75)\n",
        "    IQR = Q3 - Q1\n",
        "    fence_low = Q1 - 1.5*IQR\n",
        "    fence_high = Q3 + 1.5*IQR\n",
        "    data = data[(data[column] >= fence_low) & (data[column] <= fence_high)]\n",
        "    return data\n",
        "\n",
        "# Remove outliers from Quantity column\n",
        "online_retail = remove_outliers(online_retail, 'Quantity')\n",
        "\n",
        "# Remove outliers from UnitPrice column\n",
        "online_retail = remove_outliers(online_retail, 'UnitPrice')\n"
      ],
      "metadata": {
        "id": "Nk_h2qKOuUeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, we define a function called remove_outliers that takes in a dataframe and a column name as inputs. The function calculates the first and third quartiles (Q1 and Q3), the interquartile range (IQR), and the upper and lower fences using Tukey's method. It then filters the dataframe to keep only the rows where the column value falls within the fences. We apply this function to the Quantity and UnitPrice columns of the online_retail dataset to remove the outliers."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Create a LabelEncoder object\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Encode the 'Country' column\n",
        "online_retail['Country_Encoded'] = le.fit_transform(online_retail['Country'])\n",
        "online_retail['Country_Encoded']"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To encode the categorical columns in the given data, we can use the LabelEncoder class from the scikit-learn library. The LabelEncoder class assigns a numerical label to each unique category in a categorical column."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing \n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "!pip install contractions\n",
        "import contractions\n",
        "\n",
        "# Expand contractions in Description column\n",
        "online_retail['Description'] = online_retail['Description'].apply(lambda x: contractions.fix(x))\n"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "\n",
        "# Convert Description column to lower case\n",
        "online_retail['Description'] = online_retail['Description'].apply(lambda x: x.lower())\n",
        "online_retail['Description']"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "import string\n",
        "\n",
        "# Remove punctuations from Description column\n",
        "online_retail['Description'] = online_retail['Description'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
        "\n"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "import string\n",
        "\n",
        "# Remove punctuations from Description column\n",
        "online_retail['Description'] = online_retail['Description'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "online_retail['Description']"
      ],
      "metadata": {
        "id": "pb02-CmpyKUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# remove stopwords from Description column\n",
        "online_retail['Description'] = online_retail['Description'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\n"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "online_retail['Description'] = online_retail['Description'].str.strip()\n"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "online_retail.head()"
      ],
      "metadata": {
        "id": "bSTINQ05Knz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#rephrase text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "Sn3ydQfI1q_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenize the 'Description' column\n",
        "online_retail['Description_tokenized'] = online_retail['Description'].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "# Show the first five rows\n",
        "print(online_retail['Description_tokenized'].head())"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenize text\n",
        "text = \"This is a sample sentence, showing off the stop words filtration and lemmatization.\"\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "# Lemmatize tokens\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "online_retail['Description'] = online_retail['Description'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])\n",
        "print(lemmatizer)\n"
      ],
      "metadata": {
        "id": "qSIi0I7O3gXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization using the NLTK library: Lemmatization is the process of reducing words to their base or dictionary form, which can help to ensure that different forms of the same word are treated as the same word. For example, \"running\" and \"ran\" would both be reduced to \"run\"."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Tokenize text\n",
        "text = \"This is a sample sentence.\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Perform part-of-speech tagging\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "# Perform POS tagging on the Description column\n",
        "online_retail['Description'] = online_retail['Description'].apply(pos_tag)\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# create TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# fit and transform text data\n",
        "description_tfidf = vectorizer.fit_transform(online_retail['Description_tokenized'].apply(lambda x: ' '.join(x)))\n",
        "\n",
        "# create dataframe of vectorized data\n",
        "df_tfidf = pd.DataFrame(description_tfidf.toarray(), columns=vectorizer.get_feature_names())\n"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we use the TF-IDF vectorization technique.\n",
        "\n",
        "TF-IDF stands for Term Frequency - Inverse Document Frequency, which is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. The technique calculates a weight for each word, which reflects its importance in the document, by taking into account the number of times the word appears in the document (term frequency) and the number of documents in the corpus that contain the word (inverse document frequency)."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new features Day from Invoicedate"
      ],
      "metadata": {
        "id": "yXlShT43cgH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert InvoiceDate columns into date time format:\n",
        "online_retail['InvoiceDate'] = pd.to_datetime(online_retail['InvoiceDate'], infer_datetime_format=True)"
      ],
      "metadata": {
        "id": "GodjbZUZi8GQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create different stats from the time stamps:\n",
        "online_retail['Year'] = online_retail['InvoiceDate'].dt.year\n",
        "online_retail['Month'] = online_retail['InvoiceDate'].dt.month\n",
        "online_retail['Month_Name'] = online_retail['InvoiceDate'].dt.month_name()\n",
        "online_retail['Day'] = online_retail['InvoiceDate'].dt.day\n",
        "online_retail['Day_Name'] = online_retail['InvoiceDate'].dt.day_name()\n",
        "online_retail['Hours'] = online_retail['InvoiceDate'].dt.hour\n",
        "online_retail['Minutes'] = online_retail['InvoiceDate'].dt.minute"
      ],
      "metadata": {
        "id": "GQ-5VVLzjcJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Per Day Transaction**"
      ],
      "metadata": {
        "id": "1lrgy4-WkbGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding Per Day Transaction\n",
        "day = online_retail['Day_Name'].value_counts().reset_index()\n",
        "day.rename(columns = {'index' : 'Day'}, inplace = True)\n",
        "day.rename(columns = {'Day_Name' : 'Count'}, inplace = True)\n",
        "day"
      ],
      "metadata": {
        "id": "zNTIVJxPkZDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bar Plot Per Day Transaction:\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.title('Transaction per Day')\n",
        "sns.barplot(x='Day',y='Count',data=day)"
      ],
      "metadata": {
        "id": "VbyFszqpktDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above graph, it is clear that Thursday has higher transaction volume than the other days, possibly because clients are more available, and Friday has lower transaction volume."
      ],
      "metadata": {
        "id": "cSJn8F8Lkzuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purchasing Stats**"
      ],
      "metadata": {
        "id": "Vn2OIB8Dk3YT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot of Purchasing stats per month:\n",
        "fig, ax=plt.subplots(figsize=(15,8))\n",
        "sns.histplot(data= online_retail, x=\"Month_Name\", kde= True,ax=ax)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PeaFil_Gk04b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above graph makes it clearly transparent that buyers are purchasing winter clothing and possibly even Diwali decorations in November, which is winter season."
      ],
      "metadata": {
        "id": "2YQAkyXu9gpH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given that customers make fewer purchases in January and February and that the winter season is coming to an end, it is obvious that customers make more purchases during the winter."
      ],
      "metadata": {
        "id": "fQ-DNxCd9jTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Month = online_retail['Month_Name'].value_counts().reset_index()\n",
        "Month.rename(columns = {'index' : 'Months'}, inplace = True)\n",
        "Month.rename(columns = {'Month_Name' : 'Count'}, inplace = True)\n",
        "Month"
      ],
      "metadata": {
        "id": "KnI44yZE9Q4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bar Plot Transacton per Month:\n",
        "plt.figure(figsize=(14,7))\n",
        "plt.title('Transaction/Month', fontsize=14)\n",
        "sns.barplot(x='Months',y='Count',data = Month, palette='flare_r')\n",
        "     "
      ],
      "metadata": {
        "id": "tiPHOIpD9qEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Transaction Per Hours**"
      ],
      "metadata": {
        "id": "p1WAdpSe9tGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding Transaction Per Hours:\n",
        "Hours = online_retail['Hours'].value_counts().reset_index()\n",
        "Hours.rename(columns = {'index' : 'Hour'}, inplace = True)\n",
        "Hours.rename(columns = {'Hours' : 'Count'}, inplace = True)\n",
        "Hours"
      ],
      "metadata": {
        "id": "P03xeBnz9q-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Bar Plot Transaction Per Hours:\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.title('Transaction/Hours',fontsize= 15)\n",
        "sns.barplot(x='Hour',y='Count',data= Hours, palette='magma')"
      ],
      "metadata": {
        "id": "TG1x_DC699ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to hour, people do most of their shopping between the hours of 11 and 4, therefore practically all of them are free at this time."
      ],
      "metadata": {
        "id": "b5Z9ZtcF-C3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets distribute the day in Morining, Afternoon and Evening\n",
        "def time_type(time):\n",
        "  if (time==6 or time==7 or time==8 or time==9 or time==10 or time==11):\n",
        "    return 'Morning'\n",
        "  elif (time==12 or time==13 or time==14 or time==15 or time==16 or time==17):\n",
        "    return 'Afternoon'\n",
        "  else:\n",
        "    return 'Evening'"
      ],
      "metadata": {
        "id": "2KOH_Ez_-Hkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "online_retail['Time_type']=online_retail['Hours'].apply(time_type)\n",
        "     "
      ],
      "metadata": {
        "id": "7wZ0a0Fi-KZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot distribute the day in Morning Afternoon and Evening\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.title('Time type')\n",
        "sns.countplot(x='Time_type',data=online_retail, palette='Set2_r')"
      ],
      "metadata": {
        "id": "CgDCCoX5-WoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most customers purchase things in the afternoon, followed by modest numbers of customers in the morning, and the smallest numbers of customers in the evening."
      ],
      "metadata": {
        "id": "6k_XcbTk-adt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Correlation matrix**\n"
      ],
      "metadata": {
        "id": "mXAkyXhN-hgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting heatmap to show correlation of different variables\n",
        "plt.figure(figsize=(13,6.5))\n",
        "correlation = online_retail.corr()\n",
        "sns.heatmap(abs(correlation), annot = True, cmap = \"icefire\")\n",
        "     "
      ],
      "metadata": {
        "id": "Uk04TaHv-lqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Analyzing all Numerical Features**"
      ],
      "metadata": {
        "id": "3v5852kb_ZC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "# Assigning numerical columns to variables:\n",
        "numerical_columns = list(online_retail.select_dtypes(['int64','float64']).columns)\n",
        "numerical_features = pd.Index(numerical_columns)\n",
        "numerical_features\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysing distribution of all numerical variables:\n",
        "for col in numerical_features:\n",
        "    fig = plt.figure(figsize=(8, 4))\n",
        "    ax = fig.gca()\n",
        "    feature = online_retail[col]\n",
        "    sns.distplot(x=online_retail[col])\n",
        "    ax.axvline(feature.mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "    ax.axvline(feature.median(), color='cyan', linestyle='dashed', linewidth=2)    \n",
        "    plt.xlabel(col)\n",
        "plt.show()\n",
        "     \n"
      ],
      "metadata": {
        "id": "hO9Hlh5q_hgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse all numerical feature using histogram\n",
        "for col in numerical_features:\n",
        "  fig=plt.figure(figsize=(9,6))\n",
        "  ax=fig.gca()\n",
        "  feature= (online_retail[col])\n",
        "  feature.hist(bins=50, ax=ax)\n",
        "  ax.axvline(feature.mean(),color='magenta', linestyle='dashed', linewidth=2)\n",
        "  ax.axvline(feature.median(),color='cyan', linestyle='dashed', linewidth=2)\n",
        "  ax.set_title(col)\n",
        "  plt.show()\n",
        "  print( \"Skewness :\",online_retail[col].skew())\n",
        "  print( \"Kurtosis :\",online_retail[col].kurt())"
      ],
      "metadata": {
        "id": "T2GJg6VC_l3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BGc52rtH_Qbq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# the data don't need to transform we don't have so much feature"
      ],
      "metadata": {
        "id": "SForCkLMj7Wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why? "
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Recency, Frequency, Monetary (RFM) Model**"
      ],
      "metadata": {
        "id": "tuD-wriUB5V2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is always required to establish several quantitative factors on which the algorithm will execute segmentation prior to using any clustering techniques. These include characteristics like the total amount spent, the customer's activity level, their most recent visit, etc.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1.One of the steps is the RFM model, which stands for Recency, Frequency, and Monetary. For each customer, we calculate the recencythe number of days since the customer's last visitthe frequencyhow frequently they make repeat purchasesand the monetarytheir overall spending.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2.There are other processes where we break each of these attributes into the appropriate groups and figure out a score for each client. However, since segmentation may be done manually, this strategy does not need machine learning algorithms. As a result, we will skip the second stage and use the rfm characteristics directly as input for clustering algorithms.\n",
        "\n",
        "\n",
        "\n",
        "**Recency** :- Latest Date - Last Inovice Data\n",
        "\n",
        "**Frequency** :-Count of Invoice No. of transactions\n",
        "\n",
        "**Monetary**:- Sum of Total Amount for each customer"
      ],
      "metadata": {
        "id": "ETNTWyl0CXxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Performing RFM Segmentation and RFM Analysis**"
      ],
      "metadata": {
        "id": "56T6JQpLDs_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime as dt\n",
        "PRESENT = dt.datetime(2011,12,10)\n",
        "rfm_model= online_retail.groupby('CustomerID').agg({'InvoiceDate': lambda date: (PRESENT - date.max()).days,\n",
        "                                        'InvoiceNo': lambda num: len(num),\n",
        "                                        'Total_Price': lambda price: price.sum()})\n",
        "# Convert Invoice Date into type int\n",
        "rfm_model['InvoiceDate'] = rfm_model['InvoiceDate'].astype(int)\n",
        "\n",
        "# Rename column names to Recency, Frequency and Monetary\n",
        "rfm_model.rename(columns={'InvoiceDate': 'Recency',\n",
        "                         'InvoiceNo': 'Frequency',\n",
        "                         'Total_Price': 'Monetary'}, inplace=True)\n",
        "\n",
        "rfm_model.reset_index().head()"
      ],
      "metadata": {
        "id": "nxKO9H0xD67B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descriptive Staistics:\n",
        "rfm_model.Recency.describe()"
      ],
      "metadata": {
        "id": "OykgKSx8Egnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recency Distribution Plot\n",
        "rfm_plot = rfm_model['Recency']\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.distplot(rfm_plot)"
      ],
      "metadata": {
        "id": "p2-9aauuEl71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Desriptive Statistics (Frequency):\n",
        "rfm_plot = rfm_model['Frequency']\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.distplot(rfm_plot)"
      ],
      "metadata": {
        "id": "t-ExWOAaEoUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Computing Quantile of RFM values**"
      ],
      "metadata": {
        "id": "ry8JtutIE2Hx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Customers are categorized as top customers based on their frequency, amount spent, and recency.\n",
        "\n",
        "\n",
        "qcut() is a discretization function based on quantiles. Data is binned by qcut using sample quantiles. For instance, a categorical object representing quantile membership for each client would be produced from 1000 values for 4 quantiles."
      ],
      "metadata": {
        "id": "kngNDErMFLe3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate & Add R, F and M segment value columns in the existing dataset to show R, F and M segment values:\n",
        "rfm_model['R_quantile'] = pd.qcut(rfm_model['Recency'], 4, ['1','2','3','4'])\n",
        "rfm_model['F_quantile'] = pd.qcut(rfm_model['Frequency'], 4, ['4','3','2','1'])\n",
        "rfm_model['M_quantile'] = pd.qcut(rfm_model['Monetary'], 4, ['4','3','2','1'])\n",
        "rfm_model.head()"
      ],
      "metadata": {
        "id": "kdX5KiIqFQiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**RFM Interpretation result**"
      ],
      "metadata": {
        "id": "KB0F_gF3Fq-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate and Add column showing total sum of RFMGroup values:\n",
        "rfm_model['RFM_Score'] = rfm_model.R_quantile.astype(str)+ rfm_model.F_quantile.astype(str) + rfm_model.M_quantile.astype(str)\n",
        "rfm_model.head()\n",
        "     "
      ],
      "metadata": {
        "id": "jXsV00APF3Xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Log Transformation**"
      ],
      "metadata": {
        "id": "c8dVzgAXGCog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In order to manage infinite numbers during log transformation, handle negative and zero values:\n",
        "def handle_neg_zero(x):\n",
        "    if x <= 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return x\n",
        "# Apply the function (handle neg zero) to the Recency and Monetary columns: \n",
        "rfm_model['Recency'] = [handle_neg_zero(x) for x in rfm_model.Recency]\n",
        "rfm_model['Monetary'] = [handle_neg_zero(x) for x in rfm_model.Monetary]\n",
        "\n",
        "# Perform Log transformation to get data into a normal or nearly normal distribution:\n",
        "Log_Trans_Dist_Data = rfm_model[['Recency', 'Frequency', 'Monetary']].apply(np.log, axis = 1).round(3)\n",
        "     "
      ],
      "metadata": {
        "id": "lR7eIo3xGbmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After normalization for Recency, the distribution of data is as follows:\n",
        "Recency_Plot = Log_Trans_Dist_Data['Recency']\n",
        "plt.figure(figsize=(10,7))\n",
        "sns.distplot(Recency_Plot)\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "0MsfKctTGeeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After normalization for Frequency, the distribution of data is as follows:\n",
        "Frequency_Plot = Log_Trans_Dist_Data.query('Frequency < 1000')['Frequency']\n",
        "plt.figure(figsize=(10,7))\n",
        "sns.distplot(Frequency_Plot)"
      ],
      "metadata": {
        "id": "PIn8HJTLGiB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After normalization for Monetary, the distribution of data is as follows:\n",
        "Monetary_Plot = Log_Trans_Dist_Data.query('Monetary < 10000')['Monetary']\n",
        "plt.figure(figsize=(13,8))\n",
        "sns.distplot(Monetary_Plot)"
      ],
      "metadata": {
        "id": "LkNejtv8Gj1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**K- Means Clustring Implementation**"
      ],
      "metadata": {
        "id": "P0yXnZxdGx0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be summarized as the process of finding data subgroups where data points in the same subgroup (cluster) are extremely similar and other data points in other clusters are very different."
      ],
      "metadata": {
        "id": "enI5myEEHCjn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Finding Optimal Number of Clusters**"
      ],
      "metadata": {
        "id": "u4VFcyz_HInK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two primary methods to define number of clusters:\n",
        "\n",
        "\n",
        "1.Silhouette Score (math method)\n",
        "\n",
        "\n",
        "2.Measures intra- and inter-cluster distance\n",
        "\n",
        "\n",
        "3.Elbow criterion (visual method)\n",
        "\n",
        "\n",
        "4.Plot number of clusters against within-cluster sum-of-squared-errors (SSE) - sum of squared distances from every data point to their cluster center"
      ],
      "metadata": {
        "id": "LFyAMANvHfx3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Calculation of Silhouette Score**"
      ],
      "metadata": {
        "id": "f7INtblIIL9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating variable\n",
        "rfm_model['Recency_log'] = rfm_model['Recency'].apply(math.log)\n",
        "rfm_model['Frequency_log'] = rfm_model['Frequency'].apply(math.log)\n",
        "rfm_model['Monetary_log'] = rfm_model['Monetary'].apply(math.log)"
      ],
      "metadata": {
        "id": "vIaYGW8qIW90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Silhouette score method on Recency and Monetary**"
      ],
      "metadata": {
        "id": "xED779qaKfJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "# Applying Silhouette score method on Recency and Monetary\n",
        "features_rec_mon = ['Recency_log','Monetary_log']\n",
        "X_features_rec_mon = rfm_model[features_rec_mon].values\n",
        "\n",
        "# Fit the Algorithm\n",
        "scaler_rec_mon = preprocessing.StandardScaler()\n",
        "X_rec_mon = scaler_rec_mon.fit_transform(X_features_rec_mon)\n",
        "X=X_rec_mon\n",
        "range_n_clusters = [2,3,4,5,6,7,8,9,10,11,12,13,14]\n",
        "for n_clusters in range_n_clusters:\n",
        "  clusterer = KMeans(n_clusters=n_clusters)\n",
        "  preds = clusterer.fit_predict(X)\n",
        "  centers = clusterer.cluster_centers_\n",
        "# Predict on the model\n",
        "# Checking the silhouette score on clusters\n",
        "  score = silhouette_score(X,preds)\n",
        "  print(' For n_clusters = {}, silhouette sore is {}'.format (n_clusters, score))\n",
        "     "
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Applying Elbow Method on Recency and Monetary**"
      ],
      "metadata": {
        "id": "F_X02y5oL1Hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn import preprocessing\n",
        "\n",
        "features_rec_mon=['Recency_log','Monetary_log']\n",
        "X_features_rec_mon=rfm_model[features_rec_mon].values\n",
        "scaler_rec_mon=preprocessing.StandardScaler()\n",
        "X_rec_mon=scaler_rec_mon.fit_transform(X_features_rec_mon)\n",
        "X=X_rec_mon\n",
        "\n",
        "sum_of_sq_dist = {}\n",
        "K = range(1,10)\n",
        "for k in K:\n",
        "    kmeans = KMeans(n_clusters= k)\n",
        "    kmeans = kmeans.fit(X)\n",
        "    sum_of_sq_dist[k] = kmeans.inertia_"
      ],
      "metadata": {
        "id": "gCM2sAasL_Wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the graph for the sum of square distance values and Number of Clusters\n",
        "sns.pointplot(x = list(sum_of_sq_dist.keys()), y = list(sum_of_sq_dist.values()))\n",
        "plt.xlabel('Number of Clusters(k)')\n",
        "plt.ylabel('Sum of Square Distances')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vTmR_l6PMYqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the Cluster and Predict the values:\n",
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=2)\n",
        "kmeans.fit(X)\n",
        "y_kmeans= kmeans.predict(X)"
      ],
      "metadata": {
        "id": "HK8E7h0-MoV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot customer segmentation by taking k=2\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.title('Customer Segmentation Based on Recency and Monetary')\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='jet')\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='yellow', s=200, alpha=0.5)"
      ],
      "metadata": {
        "id": "z23JVruxMvWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we separate customers by Recency and Monetary value, we can observe that they are well-separated."
      ],
      "metadata": {
        "id": "3pcmTAooM3g_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Distance between nearest points**"
      ],
      "metadata": {
        "id": "hiSl_MXBNBKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying DBSCAN on Recency and Monetary\n",
        "y_pred = DBSCAN(eps=0.5, min_samples=15).fit_predict(X)\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "plt.scatter(X[:,0], X[:,1], c=y_pred, cmap='spring')"
      ],
      "metadata": {
        "id": "hhmtIJa6N1tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Silhouette score method on Frequency and Monetary**"
      ],
      "metadata": {
        "id": "HF6_PeCVN8kK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizing evaluation Metric Score chart**"
      ],
      "metadata": {
        "id": "uDytRton1cYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying silhouette score method on Frequency and Monetary\n",
        "features_fre_mon=['Frequency_log','Monetary_log']\n",
        "X_features_fre_mon=rfm_model[features_fre_mon].values\n",
        "scaler_fre_mon=preprocessing.StandardScaler()\n",
        "X_fre_mon=scaler_fre_mon.fit_transform(X_features_fre_mon)\n",
        "X=X_fre_mon\n",
        "range_n_clusters = [2,3,4,5,6,7,8,9,10,11,12,13,14]\n",
        "for n_clusters in range_n_clusters:\n",
        "    clusterer = KMeans(n_clusters=n_clusters)\n",
        "    preds = clusterer.fit_predict(X)\n",
        "    centers = clusterer.cluster_centers_\n",
        "\n",
        "    score = silhouette_score(X, preds)\n",
        "    print(\"For n_clusters = {}, silhouette score is {}\".format(n_clusters, score))\n",
        "     "
      ],
      "metadata": {
        "id": "f_gh0feGOFdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying elbow method on Frequency and Monetary\n",
        "sum_of_sq_dist = {}\n",
        "for k in range (1,15):\n",
        "  km = KMeans(n_clusters= k)\n",
        "  km = km.fit(X)\n",
        "  sum_of_sq_dist[k] = km.inertia_\n",
        "     "
      ],
      "metadata": {
        "id": "8Uu9ZfDwOLCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the graph for the sum of square distance values and Number of Clusters\n",
        "sns.pointplot(x = list(sum_of_sq_dist.keys()), y = list(sum_of_sq_dist.values()))\n",
        "plt.xlabel('Number of Clusters(k)')\n",
        "plt.ylabel('Sum of Square Distances')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g3Y2X3UaOPRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Hyperparameter Tuning For Best Value of K**"
      ],
      "metadata": {
        "id": "4jqZl8HbOU11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking the number of clusters as 2\n",
        "kmeans = KMeans(n_clusters=2)\n",
        "kmeans.fit(X)\n",
        "y_kmeans = kmeans.predict(X)\n",
        "     "
      ],
      "metadata": {
        "id": "OzVUiQX7Obo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot customer segmentation based on Fequency and Monetary\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.title('Customer Segmentation Based on Frquency and Monetary')\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='BrBG')\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='blue', s=200, alpha=0.5)"
      ],
      "metadata": {
        "id": "7dGmJbJLOeRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementation of Density Based Spatial Clustering of Applications with Noise (DBSCAN)**"
      ],
      "metadata": {
        "id": "KLGMP5lJP6DI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Distance between nearest points."
      ],
      "metadata": {
        "id": "mWcVqcTsP0Rz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**cross-validation and hyperparameter tuning are techniques used in supervised learning to evaluate and optimize the performance of a model. In unsupervised learning, the data is not labeled, so there is no target variable to measure performance against. Therefore, these techniques are not applicable in unsupervised learning. However, unsupervised learning algorithms may have their own parameters that can be tuned to optimize their performance so here we don't do this and we do Evaluation metric Score Chart Applying silhouette score method on Frequency and Monetary**"
      ],
      "metadata": {
        "id": "SkLW4_Z7EBcw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**DBSCAN on Recency and Monetary**"
      ],
      "metadata": {
        "id": "pEcEJOBCQeUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying DBSCAN on Recency and Monetary\n",
        "y_pred = DBSCAN(eps=0.5, min_samples=15).fit_predict(X)\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "plt.scatter(X[:,0], X[:,1], c=y_pred, cmap='spring')"
      ],
      "metadata": {
        "id": "GpMCAOtEtXT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying silhouette score method on Frequency and Monetary\n",
        "features_fre_mon=['Frequency_log','Monetary_log']\n",
        "X_features_fre_mon=rfm_model[features_fre_mon].values\n",
        "scaler_fre_mon=preprocessing.StandardScaler()\n",
        "X_fre_mon=scaler_fre_mon.fit_transform(X_features_fre_mon)\n",
        "X=X_fre_mon\n",
        "range_n_clusters = [2,3,4,5,6,7,8,9,10,11,12,13,14]\n",
        "for n_clusters in range_n_clusters:\n",
        "    clusterer = KMeans(n_clusters=n_clusters)\n",
        "    preds = clusterer.fit_predict(X)\n",
        "    centers = clusterer.cluster_centers_\n",
        "\n",
        "    score = silhouette_score(X, preds)\n",
        "    print(\"For n_clusters = {}, silhouette score is {}\".format(n_clusters, score))"
      ],
      "metadata": {
        "id": "XIDEE_OdtfA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying elbow method on Frequency and Monetary\n",
        "sum_of_sq_dist = {}\n",
        "for k in range (1,15):\n",
        "  km = KMeans(n_clusters= k)\n",
        "  km = km.fit(X)\n",
        "  sum_of_sq_dist[k] = km.inertia_\n",
        "     "
      ],
      "metadata": {
        "id": "GdybQe4VtiVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the graph for the sum of square distance values and Number of Clusters\n",
        "sns.pointplot(x = list(sum_of_sq_dist.keys()), y = list(sum_of_sq_dist.values()))\n",
        "plt.xlabel('Number of Clusters(k)')\n",
        "plt.ylabel('Sum of Square Distances')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zq-YYnO9tk3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Hyperparameter Tuning For Best Value of K**"
      ],
      "metadata": {
        "id": "M4CY7_NytqrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking the number of clusters as 2\n",
        "kmeans = KMeans(n_clusters=2)\n",
        "kmeans.fit(X)\n",
        "y_kmeans = kmeans.predict(X)"
      ],
      "metadata": {
        "id": "Qitu0cf8t41Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot customer segmentation based on Fequency and Monetary\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.title('Customer Segmentation Based on Frquency and Monetary')\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='BrBG')\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='blue', s=200, alpha=0.5)"
      ],
      "metadata": {
        "id": "moSiHHRJt-Iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**DBSCAN on Frequency and Monetary**"
      ],
      "metadata": {
        "id": "lUgw5l-QuC5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying DBSCAN method on Frequency and Monetary\n",
        "y_pred = DBSCAN(eps=0.5, min_samples=15).fit_predict(X)\n",
        "plt.figure(figsize=(13,8))\n",
        "plt.scatter(X[:,0], X[:,1], c=y_pred)"
      ],
      "metadata": {
        "id": "SvOsFhkPuE8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Comparison between R vs M and F vs M**"
      ],
      "metadata": {
        "id": "gkWpFJM2uLkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot R vs M and F vs M\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.title('R vs M and F vs M')\n",
        "plt.scatter(rfm_model.Recency_log,rfm_model.Monetary_log,alpha=0.5, color='orange')\n",
        "plt.scatter(rfm_model.Frequency_log,rfm_model.Monetary_log,alpha=0.5, color='red')"
      ],
      "metadata": {
        "id": "TJFsS9rguR3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**cross-validation and hyperparameter tuning are techniques used in supervised learning to evaluate and optimize the performance of a model. In unsupervised learning, the data is not labeled, so there is no target variable to measure performance against. Therefore, these techniques are not applicable in unsupervised learning. However, unsupervised learning algorithms may have their own parameters that can be tuned to optimize their performance so here we don't do this**"
      ],
      "metadata": {
        "id": "nQdBeXl3GA7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Agglomerative hierarchical clustering:**"
      ],
      "metadata": {
        "id": "lAGjOwK40ukn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Bottom Up Approach - We assign each point to an individual cluster in this technique. Suppose there are 4 data points. We will assign each of these points to a cluster and hence will have 4 clusters in the beginning.**\n",
        "\n",
        "\n",
        "**2. Then, at each iteration, we merge the closest pair of clusters and repeat this step until only a single cluster is left.**\n",
        "\n",
        "\n",
        "**3. We are merging (or adding) the clusters at each step, right? Hence, this type of clustering is also known as additive hierarchical clustering.**"
      ],
      "metadata": {
        "id": "CL4CD0St05U0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (18,15))\n",
        "import scipy.cluster.hierarchy as sch\n",
        "dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\n",
        "plt.title('Dendrogram',fontweight=\"bold\")\n",
        "plt.xlabel('Customers',fontweight=\"bold\")\n",
        "plt.ylabel('Euclidean Distances',fontweight=\"bold\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "omDzEE44vmvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How should we Choose the Number of Clusters in Hierarchical Clustering?**"
      ],
      "metadata": {
        "id": "YtlGbbBb1ynM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A dendrogram is a tree-like diagram that records the sequences of merges or splits.More the distance of the vertical lines in the dendrogram, more the distance between those clusters.**\n",
        "\n",
        "**We can set a threshold distance and draw a horizontal line (Generally, we try to set the threshold in such a way that it cuts the tallest vertical line).**"
      ],
      "metadata": {
        "id": "CmRd5rt12g-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**We choose threshold value to be 70 and 40 and see the difference in no. of clusters.**"
      ],
      "metadata": {
        "id": "jUgLYgRo2wlj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set threshold value to be 70**"
      ],
      "metadata": {
        "id": "i5xJGt0Z26PO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "plt.figure(figsize = (18,15))\n",
        "dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\n",
        "plt.title('Dendrogram',fontweight=\"bold\")\n",
        "plt.xlabel('Customers',fontweight=\"bold\")\n",
        "plt.ylabel('Euclidean Distances',fontweight=\"bold\")\n",
        "plt.axhline(y=70, color='r', linestyle='--')\n",
        "plt.show() # find largest vertical distance we can make without crossing any other horizontal line\n",
        "\n",
        "# Fit the Algorithm\n",
        "# Fitting hierarchical clustering to the mall dataset\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "hc = AgglomerativeClustering(n_clusters = 2, affinity = 'euclidean', linkage = 'ward')\n",
        "y_hc = hc.fit_predict(X)\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Conclusion:**"
      ],
      "metadata": {
        "id": "PM1qTWaI34Fz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.The number of clusters will be the number of vertical lines which are being intersected by the line drawn using the threshold.\n",
        "\n",
        "2.Here,No. of Cluster = 2"
      ],
      "metadata": {
        "id": "2Wmc8SgP4CIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting hierarchical clustering to the mall dataset\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "hc = AgglomerativeClustering(n_clusters = 2, affinity = 'euclidean', linkage = 'ward')\n",
        "y_hc = hc.fit_predict(X)"
      ],
      "metadata": {
        "id": "Y1rTc0gX4Kjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the clusters (two dimensions only)\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 20, c = 'red', label = 'Type 1 Customer')\n",
        "plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 20, c = 'blue', label = 'Type 2 Customer')\n",
        "plt.title(\"Visualizing the clusters\",fontweight=\"bold\")\n",
        "plt.xlabel('RFM',fontweight=\"bold\")\n",
        "plt.ylabel('Types of customer',fontweight=\"bold\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1Isd9Tgn4P7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion:**\n",
        "\n",
        "1.Clusters data points are well-separated.\n",
        "\n",
        "2.is optimal no. of clusters"
      ],
      "metadata": {
        "id": "piYLliaz4nlw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Set threshold value to be 40**"
      ],
      "metadata": {
        "id": "I629OqTz5D6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (18,15))\n",
        "dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\n",
        "plt.title('Dendrogram',fontweight=\"bold\")\n",
        "plt.xlabel('Customers',fontweight=\"bold\")\n",
        "plt.ylabel('Euclidean Distances',fontweight=\"bold\")\n",
        "plt.axhline(y=40, color='r', linestyle='--')\n",
        "plt.show() # find largest vertical distance we can make without crossing any other horizontal line"
      ],
      "metadata": {
        "id": "zHZtTMMG44wR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "hc = AgglomerativeClustering(n_clusters = 2, affinity = 'euclidean', linkage = 'ward')\n",
        "y_hc = hc.fit_predict(X)\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 20, c = 'red', label = 'Type 1 Customer')\n",
        "plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 20, c = 'blue', label = 'Type 2 Customer')\n",
        "plt.title(\"Visualizing the clusters\",fontweight=\"bold\")\n",
        "plt.xlabel('RFM',fontweight=\"bold\")\n",
        "plt.ylabel('Types of customer',fontweight=\"bold\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting hierarchical clustering to the online retail dataset\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "hc = AgglomerativeClustering(n_clusters = 3, affinity = 'euclidean', linkage = 'ward')\n",
        "y_hc = hc.fit_predict(X)"
      ],
      "metadata": {
        "id": "NKzHimAi5NR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the clusters (two dimensions only)\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 20, c = 'red', label = 'Type 1 Customer')\n",
        "plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 20, c = 'blue', label = 'Type 2 Customer')\n",
        "plt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 20, c = 'green', label = 'Type 3 Customer')\n",
        "plt.title(\"Visualizing the clusters \")\n",
        "plt.xlabel('RFM',fontweight=\"bold\")\n",
        "plt.ylabel('Types of customers',fontweight=\"bold\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yQHI3nXB5QDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion:**"
      ],
      "metadata": {
        "id": "T5tqsPdq5hRJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above dotted graph:\n",
        "\n",
        "1.When no. of cluster is 3 ,then some datapoints in different clusters are overlapping with each other.\n",
        "\n",
        "\n",
        "2.They are not well seperated."
      ],
      "metadata": {
        "id": "fn6TOsXv5omB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Types of Linkage:**"
      ],
      "metadata": {
        "id": "CFWonZDs5-h2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.cluster.hierarchy import linkage"
      ],
      "metadata": {
        "id": "xILEFtq66Fxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Single Linkage Hierarchical Clustering :-**\n",
        "In single linkage hierarchical clustering, the distance between two clusters is defined as the shortest distance between two points in each cluster. For example, the distance between clusters r and s to the left is equal to the length of the arrow between their two closest points."
      ],
      "metadata": {
        "id": "hzZ3i-926JvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Single linkage plot for dendogram\n",
        "plt.figure(figsize = (18,15))\n",
        "dendrogram = sch.dendrogram(sch.linkage(X, method = 'single'))\n",
        "plt.title('Single Linkage Dendrogram',fontweight=\"bold\")\n",
        "plt.xlabel('Customers',fontweight=\"bold\")\n",
        "plt.ylabel('Euclidean Distances',fontweight=\"bold\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xjNtDSEs6YFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Complete Linkage Hierarchical Clustering :-**\n",
        "In complete linkage hierarchical clustering, the distance between two clusters is defined as the longest distance between two points in each cluster. For example, the distance between clusters r and s to the left is equal to the length of the arrow between their two furthest points."
      ],
      "metadata": {
        "id": "iMrHEs4W6gPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete linkage plot for dendrogram\n",
        "plt.figure(figsize = (18,15))\n",
        "dendrogram = sch.dendrogram(sch.linkage(X, method = 'complete'))\n",
        "plt.title('Complete Linkage Dendrogram',fontweight=\"bold\")\n",
        "plt.xlabel('Customers',fontweight=\"bold\")\n",
        "plt.ylabel('Euclidean Distances',fontweight=\"bold\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sVJx100p6nOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Average Linkage Hierarchical Clustering :-**\n",
        "In average linkage hierarchical clustering, the distance between two clusters is defined as the average distance between each point in one cluster to every point in the other cluster. For example, the distance between clusters r and s to the left is equal to the average length each arrow between connecting the points of one cluster to the other."
      ],
      "metadata": {
        "id": "oU8i-yto6vQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Average linkage plot for dendrogram\n",
        "plt.figure(figsize = (18,15))\n",
        "dendrogram = sch.dendrogram(sch.linkage(X, method = 'average'))\n",
        "plt.title('Average Linkage Dendrogram',fontweight=\"bold\")\n",
        "plt.xlabel('Customers',fontweight=\"bold\")\n",
        "plt.ylabel('Euclidean Distances',fontweight=\"bold\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hBtpRMJ-662P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Centroid Linkage Hierarchical Clustering :-**\n",
        "In centroid linkage hierarchical clustering, the distance between two clusters is the distance between the two mean vectors of the clusters. At each stage of the process we combine the two clusters that have the smallest centroid distance.For example, the distance between clusters r and s to the left is equal to the length of the arrow between their respective centroids."
      ],
      "metadata": {
        "id": "QnOydHqK6-vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Centroid linkage plot for dendrogram\n",
        "plt.figure(figsize = (18,15))\n",
        "dendrogram = sch.dendrogram(sch.linkage(X, method = 'centroid'))\n",
        "plt.title('Centroid Linkage Dendrogram',fontweight=\"bold\")\n",
        "plt.xlabel('Customers',fontweight=\"bold\")\n",
        "plt.ylabel('Euclidean Distances',fontweight=\"bold\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cD_Tu3Vd7Jkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Final Table**"
      ],
      "metadata": {
        "id": "_sFBaYCH7Ohe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable \n",
        "# Specify the Column Names while initializing the Table \n",
        "finalTable = PrettyTable(['SL No.',\"Model_Name\",'Data', \"Optimal_Number_of_cluster\"]) \n",
        "# Add rows \n",
        "finalTable.add_row(['1',\"K-Means with silhouette_score \", \"RFM\", \"2\"]) \n",
        "finalTable.add_row(['2',\"K-Means with Elbow method  \", \"RFM\", \"2\"])\n",
        "finalTable.add_row(['3',\"Agglomerative Hierarchical Clustering with threshold value 70\", \"RFM\", \"2\"])\n",
        "finalTable.add_row(['4',\"Agglomerative Hierarchical Clustering with threshold value 40\", \"RFM\", \"3\"])\n",
        "print(finalTable)"
      ],
      "metadata": {
        "id": "blM8pPsE7TcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**cross-validation and hyperparameter tuning are techniques used in supervised learning to evaluate and optimize the performance of a model. In unsupervised learning, the data is not labeled, so there is no target variable to measure performance against. Therefore, these techniques are not applicable in unsupervised learning. However, unsupervised learning algorithms may have their own parameters that can be tuned to optimize their performance so here we don't do this and we do Evaluation metric Score Chart Applying silhouette score method on Frequency and Monetary**"
      ],
      "metadata": {
        "id": "Uom_eeOAGZgO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**These are the model we perform**\n",
        "\n",
        "1.K- Means Clustring Implementation\n",
        "\n",
        "2.DBSCAN on Recency and Monetary\n",
        "\n",
        "3.Agglomerative hierarchical clustering:"
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.K-Means algorithm groups customers based on their purchasing behavior into K clusters. The number of clusters is determined using the elbow method or silhouette score. Characteristics of each cluster, such as average spending and frequently purchased items, are analyzed after clustering.\n",
        "\n",
        "2.DBSCAN algorithm identifies outliers or noise points based on the Recency and Monetary features by grouping data points based on their density. It groups close data points together and separates the ones that are far away.\n",
        "\n",
        "3.Agglomerative hierarchical clustering algorithm groups customers based on their purchasing behavior by merging clusters based on their similarity. The number of clusters is determined using the dendrogram or silhouette score."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.We performed consumer segmentation using a variety of steps\n",
        "throughout the analysis. Starting with data wrangling, we tried to deal with duplicates, null values, and feature updates. We then performed some exploratory data analysis in an effort to derive observations from the dataset's features.\n",
        "\n",
        "2.Then, for each of the consumers, we developed some quantitative components, such as recency, frequency, and monetary data, known as the rfm model. On these features, we applied the KMeans clustering algorithm. To determine the ideal number of clusters, which was 2, we also performed silhouette and elbow method analyses.\n",
        "\n",
        "3.Customers with low frequency and high value transactions were part of one cluster, while those with low frequency and high value transactions were part of another cluster.\n",
        "\n",
        "4.There may be other adjustments made to this analysis, though. Depending on the goals and preferences of the firm, one may decide to cluster into a greater number. After clustering, the tagged feature can be put into supervised machine learning algorithms for classification that can forecast the classes for fresh sets of observations.\n",
        "\n",
        "5.The clustering can also be done on a new set of features, such segmenting customers based on the times of their visits, determining customer lifetime value (CLV), and many more.\n",
        "\n",
        "6.We have use Agglomerative Hierarchical Clustering with different threshold value and see how clusters differ and find optimal no. of clustes.\n",
        "\n",
        "7.The optimal no. of clusters with threshold value 70 in Agglomerative clustering is 2.\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}